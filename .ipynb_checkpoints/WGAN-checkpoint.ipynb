{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some modifications:\n",
    "1. Use SELU as activation function instead of RELU.\n",
    "\n",
    "Two quesitons remain:\n",
    "1. Do we need to clamp the parameter of Generator?\n",
    "2. Is the loss WGAN trying to optimize equivalent to Eearh-over Distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "# This code follows the instruction of https://github.com/martinarjovsky/WassersteinGAN/blob/master/main.py\n",
    "# Notice how to freeze G and D respectively.\n",
    "# When training D, let G be volatile, and set all parameters in D to False.\n",
    "# When training G, set all parameters of D to False.\n",
    "# The reason doing so is that when training D, we don't need any gradient information \n",
    "# concerning G, but it is not the case when we train G, in which scenario we have to use \n",
    "# the gradient back-propagated by D.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "from model import *\n",
    "\n",
    "n_epochs = 25\n",
    "init_Diters = 5\n",
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "beta1 = 0.5\n",
    "clamp_lower = -0.01\n",
    "clamp_upper = 0.01\n",
    "batch_size = 64\n",
    "ngpu = 2\n",
    "nz = 50\n",
    "nc = 1\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "isize = 28\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "G = Generator(nz, nc, ngf, isize, ngpu)\n",
    "D = Discriminator(nz, nc, ndf, isize, ngpu)\n",
    "G.cuda()\n",
    "D.cuda()\n",
    "\n",
    "optimizerD = optim.RMSProp(D.parameters(), lr = lrD, betas=(beta1, 0.999))\n",
    "optimizerG = optim.RMSProp(G.parameters(), lr = lrG, betas=(beta1, 0.999))\n",
    "\n",
    "input = torch.FloatTensor(batch_size, nc, isize, isize)\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = torch.FloatTensor([-1])\n",
    "input.cuda()\n",
    "noise.cuda()\n",
    "one.cuda()\n",
    "mone.cuda()\n",
    "\n",
    "\n",
    "def set_require_grads(net, val=True):\n",
    "    for p in net.parameters():\n",
    "        p.requires_grad = val\n",
    "\n",
    "# The reason we use clamp_ rather than clamp here is that\n",
    "# p.clamp will return a new Variable but it will not change p itself.\n",
    "# p.clamp_ is an in-place version and it will clamp p.\n",
    "def clamp_grads(net, lower, upper):\n",
    "    for p in net.parameters():\n",
    "        p.data.clamp_(lower, upper)\n",
    "\n",
    "# The main loop is a little bit different from the vanilla NN training pipeline for the reason\n",
    "# that we need to train D several times as we train G once.\n",
    "\n",
    "g_iters = 0\n",
    "for epoch in xrange(n_epochs):\n",
    "    i = 0\n",
    "    data_iter = iter(train_loader) # Bulid a new iterator in each epoch.\n",
    "    \n",
    "    while i < len(data_iter):\n",
    "        if g_iters < 25 or g_iters % 500 == 0:\n",
    "            D_iters = 100\n",
    "        else:\n",
    "            D_iters = init_Diters\n",
    "        \n",
    "        # Train D\n",
    "        set_require_grads(D, True)\n",
    "        j = 0\n",
    "        while i < len(data_iter) and j < D_iters:\n",
    "            j += 1\n",
    "            clamp_grads(D)\n",
    "            i, (real_cpu, _) = i + 1, data_iter.next()\n",
    "            \n",
    "            D.zero_grad()\n",
    "            # \n",
    "            input.resize_as_(real_cpu).cpu_(real_cpu)\n",
    "            inputv = Variable(input)\n",
    "            err_real = D(inputv)\n",
    "            err_real.backward(one)\n",
    "            \n",
    "            noise.normal_(0, 1)\n",
    "            noisev = Variable(noise, volatile=True)\n",
    "            fake = G(noisev)\n",
    "            err_fake = D(Variable(fake.data))\n",
    "            err_fake.backward(mone)\n",
    "            err_d = err_real - err_fake\n",
    "            optimizerD.step()\n",
    "        \n",
    "        # Train G        \n",
    "        set_require_grads(D, False)\n",
    "        G.zero_grad()\n",
    "        \n",
    "        noise.normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = G(noisev)\n",
    "        err_g = D(fake)\n",
    "        err_g.backward(one)\n",
    "        g_iters += 1\n",
    "        optimizerG.step()\n",
    "        \n",
    "        print('epoch: {}/{}, batches: {}/{}, Loss_D, Loss_G, Loss_D_real, Loss_D_fake'.format(\n",
    "            epoch, n_epochs, i, len(train_loader), \n",
    "            err_d.data[0], err_g.data[0], err_real.data[0], err_fake.data[0]))\n",
    "    \n",
    "    torch.save(G.state_dict(), 'params/generator.params')\n",
    "    torch.save(D.state_dict(), 'params/discriminator.params')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Replace the original activation layer with SELU function.\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, nc, ngf, isize, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        net = nn.Sequential(\n",
    "            nn.Linear(nz, ngf),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(ngf, ngf),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(ngf, ngf),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(ngf, nc * isize * isize)\n",
    "        )\n",
    "        self.net = net\n",
    "        self.nz = nz\n",
    "        self.isize = isize\n",
    "        self.nc = nc\n",
    "        self.ngpu = ngpu\n",
    "        self.para = torch.nn.DataParallel(self.net, device_ids=range(self.ngpu))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.para(input)\n",
    "        return output.view(output.size(0), self.nc, self.isize, self.isize)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nz, nc, ndf, isize, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        net = nn.Sequential(\n",
    "            nn.Linear(nc * isize * isize, ndf),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(ndf, ndf),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(ndf, ndf),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(ndf, 1)\n",
    "        )\n",
    "        self.net = net\n",
    "        self.nz = nz\n",
    "        self.nc = nc\n",
    "        self.ndf = ndf\n",
    "        self.isize = isize\n",
    "        self.ngpu = ngpu\n",
    "        self.para = torch.nn.DataParallel(self.net, device_ids=range(self.ngpu))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = input.view(input.size(0), -1)\n",
    "        output = self.para(input)\n",
    "        output = output.mean(0) # The reason doing so is that we only pass a batch of true samples/bad samples.\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
